# @file web_search_with_rag.py
# @summary Webæ¤œç´¢çµæœã‚’RAGåŒ–ã™ã‚‹æ‹¡å¼µãƒ„ãƒ¼ãƒ«
# @responsibility Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ä¸€æ™‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜ã—ã¾ã™

from langchain.tools import tool
from src.core.logger import logger
from src.llm.rag.collection_manager import CollectionManager
from src.llm.rag.document_processor import DocumentProcessor
from typing import Optional
import os
import httpx
import asyncio
from bs4 import BeautifulSoup  # type: ignore
from datetime import datetime


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_collection_manager: Optional[CollectionManager] = None
_document_processor: Optional[DocumentProcessor] = None


def get_collection_manager() -> CollectionManager:
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—

    Returns:
        CollectionManager: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    global _collection_manager
    if _collection_manager is None:
        _collection_manager = CollectionManager()
        logger.info("CollectionManager instance created for web_search_with_rag tool")
    return _collection_manager


def get_document_processor() -> DocumentProcessor:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—

    Returns:
        DocumentProcessor: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    global _document_processor
    if _document_processor is None:
        _document_processor = DocumentProcessor(
            chunk_size=1000,
            chunk_overlap=200
        )
        logger.info("DocumentProcessor instance created for web_search_with_rag tool")
    return _document_processor


@tool
async def web_search_with_rag(
    query: str,
    max_results: int = 5,
    collection_ttl_hours: float = 1.0
) -> str:
    """
    Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ä¸€æ™‚RAGã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ä¿å­˜ã—ã¾ã™ã€‚

    ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
    1. Google Custom Search APIã§Webæ¤œç´¢ã‚’å®Ÿè¡Œ
    2. ä¸Šä½çµæœã®ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å…¨æ–‡å–å¾—
    3. ä¸€æ™‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆTTL: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1æ™‚é–“ï¼‰
    4. å–å¾—ã—ãŸHTMLãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦RAGã«ä¿å­˜
    5. ä½œæˆã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‚’è¿”ã™

    è¿”ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‚’ä½¿ã£ã¦ã€search_knowledge_baseãƒ„ãƒ¼ãƒ«ã§
    æ·±ã„ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

    ä½¿ç”¨ä¾‹:
    1. ã“ã®ãƒ„ãƒ¼ãƒ«ã§Webæ¤œç´¢çµæœã‚’RAGåŒ–: web_search_with_rag("Python async best practices")
    2. è¿”ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‚’ä½¿ã£ã¦æ·±ã„æ¤œç´¢: search_knowledge_base("asyncio event loop", collection_name="web_1762386000")

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆä¾‹: "æ©Ÿæ¢°å­¦ç¿’ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰", "FastAPI ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"ï¼‰
        max_results: å–å¾—ã™ã‚‹æ¤œç´¢çµæœã®æœ€å¤§æ•°ï¼ˆ1-10ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
        collection_ttl_hours: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®TTLï¼ˆæ™‚é–“å˜ä½ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰

    Returns:
        ä½œæˆã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã¨æ¤œç´¢çµæœã‚µãƒãƒªãƒ¼ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    logger.info(
        f"web_search_with_rag tool called: query={query}, "
        f"max_results={max_results}, ttl={collection_ttl_hours}h"
    )

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
    max_results = max(1, min(10, max_results))
    collection_ttl_hours = max(0.1, min(24.0, collection_ttl_hours))

    # Google Custom Search APIã®ã‚­ãƒ¼ç¢ºèª
    api_key = os.getenv("GOOGLE_API_KEY")
    search_engine_id = os.getenv("GOOGLE_CSE_ID")

    if not api_key or not search_engine_id:
        error_msg = "Google Custom Search APIã®è¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"
        logger.error(
            f"Missing API credentials: api_key={bool(api_key)}, "
            f"search_engine_id={bool(search_engine_id)}"
        )
        return (
            f"ã‚¨ãƒ©ãƒ¼: {error_msg}\n\n"
            ".envãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„:\n"
            "- GOOGLE_API_KEY: Google API Key\n"
            "- GOOGLE_CSE_ID: Custom Search Engine ID"
        )

    try:
        # 1. Google Custom Search APIã§æ¤œç´¢å®Ÿè¡Œ
        async with httpx.AsyncClient(timeout=30.0) as client:
            url = "https://www.googleapis.com/customsearch/v1"
            params: dict[str, str | int] = {
                "key": api_key,
                "cx": search_engine_id,
                "q": query,
                "num": max_results,
                "lr": "lang_ja",  # æ—¥æœ¬èªã®çµæœã‚’å„ªå…ˆ
            }

            response = await client.get(url, params=params)
            response.raise_for_status()
            data = response.json()

            if "items" not in data or len(data["items"]) == 0:
                return f"æ¤œç´¢çµæœ: ã‚¯ã‚¨ãƒª '{query}' ã«ä¸€è‡´ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

            # 2. å…¨ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ä¸¦åˆ—å–å¾—
            search_results = data["items"][:max_results]
            logger.info(f"Fetching full content from {len(search_results)} URLs")

            # URLã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            fetch_tasks = []
            for i, result in enumerate(search_results, 1):
                url_to_fetch = result.get('link')
                if url_to_fetch:
                    metadata = {
                        "url": url_to_fetch,
                        "title": result.get('title', '(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)'),
                        "snippet": result.get('snippet', ''),
                        "search_query": query,
                        "search_rank": i,
                        "source": "google_web_search",
                        "fetched_at": datetime.now().isoformat()
                    }
                    fetch_tasks.append(_fetch_and_extract(url_to_fetch, metadata))

            # ä¸¦åˆ—ã«ãƒšãƒ¼ã‚¸å–å¾—ã¨æŠ½å‡ºã‚’å®Ÿè¡Œ
            page_data_list = await asyncio.gather(*fetch_tasks, return_exceptions=True)

            # æˆåŠŸã—ãŸãƒšãƒ¼ã‚¸ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
            successful_pages = []
            for page_data in page_data_list:
                if isinstance(page_data, dict) and page_data.get("text"):
                    successful_pages.append(page_data)
                    logger.info(
                        f"Successfully fetched and extracted: {page_data['metadata']['url']} "
                        f"({len(page_data['text'])} chars)"
                    )

            if not successful_pages:
                return (
                    f"ã‚¨ãƒ©ãƒ¼: ã‚¯ã‚¨ãƒª '{query}' ã§æ¤œç´¢çµæœã¯è¦‹ã¤ã‹ã‚Šã¾ã—ãŸãŒã€"
                    "ãƒšãƒ¼ã‚¸å†…å®¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n"
                    "ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã§ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                )

            # 3. ä¸€æ™‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
            manager = get_collection_manager()
            collection_name = manager.generate_temp_collection_name("web")
            vector_store = manager.create_collection(
                name=collection_name,
                collection_type="temp",
                ttl_hours=collection_ttl_hours,
                description=f"Web search results for: {query}"
            )

            logger.info(
                f"Created temporary collection: {collection_name} "
                f"(TTL: {collection_ttl_hours}h)"
            )

            # 4. DocumentProcessorã§ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†
            processor = get_document_processor()
            all_chunks = []

            for page_data in successful_pages:
                text = page_data["text"]
                metadata = page_data["metadata"]

                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–
                chunks = processor.load_from_text(text, metadata=metadata)
                all_chunks.extend(chunks)

                logger.debug(
                    f"Processed {metadata['url']}: {len(chunks)} chunks created"
                )

            # 5. VectorStoreã«è¿½åŠ 
            vector_store.add_documents(all_chunks, save_after_add=True)

            logger.info(
                f"Added {len(all_chunks)} chunks from {len(successful_pages)} pages "
                f"to collection '{collection_name}'"
            )

            # 6. çµæœã‚µãƒãƒªãƒ¼ã‚’è¿”ã™
            return _format_rag_result(
                query=query,
                collection_name=collection_name,
                pages_count=len(successful_pages),
                chunks_count=len(all_chunks),
                ttl_hours=collection_ttl_hours,
                search_results=search_results[:len(successful_pages)]
            )

    except httpx.HTTPStatusError as e:
        error_msg = f"HTTP {e.response.status_code}: {e.response.text}"
        logger.error(f"HTTP error in web_search_with_rag: query={query}, error={error_msg}")

        if e.response.status_code == 429:
            return "ã‚¨ãƒ©ãƒ¼: æ¤œç´¢ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        elif e.response.status_code == 403:
            return "ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒç„¡åŠ¹ã‹ã€æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        else:
            return f"ã‚¨ãƒ©ãƒ¼: Webæ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ (HTTP {e.response.status_code})"

    except httpx.TimeoutException:
        logger.error(f"Timeout in web_search_with_rag: query={query}")
        return "ã‚¨ãƒ©ãƒ¼: æ¤œç´¢ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚å¾Œã§ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

    except Exception as e:
        error_msg = str(e)
        logger.error(f"Error in web_search_with_rag: query={query}, error={error_msg}")
        return f"ã‚¨ãƒ©ãƒ¼: Webæ¤œç´¢RAGåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg}"


async def _fetch_and_extract(url: str, metadata: dict) -> dict:
    """URLã‹ã‚‰HTMLã‚’å–å¾—ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º

    Args:
        url: å–å¾—ã™ã‚‹URL
        metadata: ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

    Returns:
        {"text": str, "metadata": dict} ã¾ãŸã¯ç©ºè¾æ›¸ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
    """
    try:
        # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            response.raise_for_status()
            html = response.text

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆåˆ¶é™ãªã— - ãƒãƒ£ãƒ³ã‚¯åŒ–ã«ä»»ã›ã‚‹ï¼‰
        text = _extract_text_from_html(html, max_length=None)

        if not text:
            logger.warning(f"No text extracted from {url}")
            return {}

        return {
            "text": text,
            "metadata": metadata
        }

    except httpx.TimeoutException:
        logger.warning(f"Timeout fetching URL: {url}")
        return {}
    except httpx.HTTPStatusError as e:
        logger.warning(f"HTTP error {e.response.status_code} fetching URL: {url}")
        return {}
    except Exception as e:
        logger.warning(f"Error fetching and extracting {url}: {str(e)}")
        return {}


def _extract_text_from_html(html: str, max_length: Optional[int] = None) -> str:
    """HTMLã‹ã‚‰æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º

    Args:
        html: HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        max_length: æŠ½å‡ºã™ã‚‹æœ€å¤§æ–‡å­—æ•°ï¼ˆNoneã®å ´åˆã¯åˆ¶é™ãªã—ï¼‰

    Returns:
        æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        soup = BeautifulSoup(html, 'html.parser')

        # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'noscript']):
            tag.decompose()

        # æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆå„ªå…ˆé †ä½: article > main > bodyï¼‰
        main_content = (
            soup.find('article') or
            soup.find('main') or
            soup.find('body') or
            soup
        )

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦æ•´å½¢
        text = main_content.get_text(separator='\n', strip=True)

        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’å‰Šé™¤
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        text = '\n'.join(lines)

        # é•·ã•åˆ¶é™ï¼ˆæŒ‡å®šã•ã‚ŒãŸå ´åˆï¼‰
        if max_length is not None and len(text) > max_length:
            text = text[:max_length] + "..."

        return text

    except Exception as e:
        logger.warning(f"Error extracting text from HTML: {str(e)}")
        return ""


def _format_rag_result(
    query: str,
    collection_name: str,
    pages_count: int,
    chunks_count: int,
    ttl_hours: float,
    search_results: list
) -> str:
    """RAGåŒ–çµæœã‚’æ•´å½¢

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        collection_name: ä½œæˆã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        pages_count: å–å¾—ã—ãŸãƒšãƒ¼ã‚¸æ•°
        chunks_count: ä½œæˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°
        ttl_hours: TTLï¼ˆæ™‚é–“ï¼‰
        search_results: Googleæ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ

    Returns:
        æ•´å½¢ã•ã‚ŒãŸçµæœæ–‡å­—åˆ—
    """
    result_parts = [
        f"âœ“ Webæ¤œç´¢çµæœã‚’RAGã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ä¿å­˜ã—ã¾ã—ãŸ\n",
        f"\n{'='*60}\n",
        f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {query}\n",
        f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: {collection_name}\n",
        f"ä¿å­˜ã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°: {pages_count}\n",
        f"ä½œæˆã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ•°: {chunks_count}\n",
        f"æœ‰åŠ¹æœŸé™: {ttl_hours}æ™‚é–“å¾Œã«è‡ªå‹•å‰Šé™¤\n",
        f"\n{'='*60}\n",
        f"\nå–å¾—ã—ãŸãƒšãƒ¼ã‚¸:\n"
    ]

    for i, result in enumerate(search_results, 1):
        title = result.get('title', '(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)')
        url = result.get('link', '')
        snippet = result.get('snippet', '')

        result_parts.append(f"\n{i}. {title}")
        result_parts.append(f"\n   URL: {url}")
        if snippet:
            snippet_text = snippet[:150] + "..." if len(snippet) > 150 else snippet
            result_parts.append(f"\n   æ¦‚è¦: {snippet_text}")

    result_parts.append(f"\n\n{'='*60}\n")
    result_parts.append(
        f"\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n"
        f"search_knowledge_baseãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã€ã“ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å†…ã‚’æ¤œç´¢ã§ãã¾ã™ã€‚\n"
        f"ä¾‹: search_knowledge_base(\n"
        f"    query=\"å…·ä½“çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒª\",\n"
        f"    collection_name=\"{collection_name}\"\n"
        f")\n"
    )

    return "".join(result_parts)
