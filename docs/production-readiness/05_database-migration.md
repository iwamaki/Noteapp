# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç§»è¡Œè¨ˆç”»

**å„ªå…ˆåº¦**: ğŸ”´ CRITICAL
**æ¨å®šä½œæ¥­æœŸé–“**: 1é€±é–“
**å¯¾è±¡**: SQLite â†’ PostgreSQL + Alembicå°å…¥

## ğŸ“Š ç¾çŠ¶ã®å•é¡Œ

### ç¾åœ¨ã®DBæ§‹æˆ

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: SQLite 3
**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/src/billing/infrastructure/persistence/database.py:16`

```python
DATABASE_URL = "sqlite:///./billing.db"

# é–‹ç™ºç”¨ã®åˆæœŸåŒ–æ–¹æ³•
Base.metadata.create_all(bind=engine)
```

### ğŸš¨ SQLiteã®å•é¡Œç‚¹

#### 1. åŒæ™‚æ›¸ãè¾¼ã¿åˆ¶é™

**å•é¡Œ**:
- SQLiteã¯å˜ä¸€ãƒ©ã‚¤ã‚¿ãƒ¼ãƒ­ãƒƒã‚¯
- åŒæ™‚ã«1ã¤ã®æ›¸ãè¾¼ã¿ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿
- ä»–ã®æ›¸ãè¾¼ã¿ã¯å¾…æ©Ÿï¼ˆãƒ­ãƒƒã‚¯å¾…ã¡ï¼‰

**å½±éŸ¿**:
```python
# ãƒ¦ãƒ¼ã‚¶ãƒ¼AãŒãƒˆãƒ¼ã‚¯ãƒ³è³¼å…¥ä¸­
# â†’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒãƒ­ãƒƒã‚¯

# ãƒ¦ãƒ¼ã‚¶ãƒ¼Bã‚‚ãƒˆãƒ¼ã‚¯ãƒ³è³¼å…¥ã—ã‚ˆã†ã¨ã™ã‚‹
# â†’ "database is locked" ã‚¨ãƒ©ãƒ¼
```

#### 2. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æ¬ å¦‚

**å•é¡Œ**:
- æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸å¯èƒ½
- è¤‡æ•°ã‚µãƒ¼ãƒãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹éå¯¾å¿œ
- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®åˆ¶ç´„

#### 3. ãƒ‡ãƒ¼ã‚¿æå¤±ãƒªã‚¹ã‚¯

**å•é¡Œ**:
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®éšœå®³ = ãƒ‡ãƒ¼ã‚¿å…¨æå¤±
- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒæ‰‹å‹•
- PITRï¼ˆPoint-in-Time Recoveryï¼‰ä¸å¯

#### 4. æœ¬ç•ªç’°å¢ƒã§ã®æ¨å¥¨ãªã—

**å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
> "SQLite is not recommended for production use in web applications with concurrent writes."

---

## ğŸ¯ ç§»è¡Œç›®æ¨™

### Phase 1: PostgreSQLç§»è¡Œ
- SQLite â†’ PostgreSQL
- æ¥ç¶šãƒ—ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
- ç’°å¢ƒåˆ¥DBè¨­å®š

### Phase 2: Alembicå°å…¥
- ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½

### Phase 3: ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
- æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
- æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯

---

## ğŸ˜ PostgreSQLç’°å¢ƒæ§‹ç¯‰

### 1. ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒ

#### Docker Composeã«ã‚ˆã‚‹æ§‹ç¯‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/docker-compose.yml`ï¼ˆæ–°è¦ä½œæˆï¼‰

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    container_name: noteapp-postgres-dev
    environment:
      POSTGRES_USER: noteapp_user
      POSTGRES_PASSWORD: noteapp_dev_password
      POSTGRES_DB: noteapp_dev
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U noteapp_user"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: noteapp-redis-dev
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
  redis_data:
```

#### åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/init-db.sql`ï¼ˆæ–°è¦ä½œæˆï¼‰

```sql
-- Extension for UUID support
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Set timezone
SET timezone = 'UTC';

-- Create default user (will be managed by Alembic later)
-- Tables will be created by Alembic migrations
```

#### èµ·å‹•ãƒ»åœæ­¢

```bash
# èµ·å‹•
cd server
docker-compose up -d

# åœæ­¢
docker-compose down

# ãƒ­ã‚°ç¢ºèª
docker-compose logs -f postgres

# ã‚³ãƒ³ãƒ†ãƒŠå†æ§‹ç¯‰
docker-compose down -v  # ãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤
docker-compose up -d
```

---

### 2. æœ¬ç•ªç’°å¢ƒï¼ˆGCP Cloud SQLï¼‰

#### Cloud SQL ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ

```bash
# GCP ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
gcloud config set project YOUR_PROJECT_ID

# PostgreSQL ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
gcloud sql instances create noteapp-prod \
  --database-version=POSTGRES_15 \
  --tier=db-f1-micro \  # æœ¬ç•ªã¯ db-custom-2-7680 ç­‰ã«å¤‰æ›´
  --region=asia-northeast1 \
  --storage-type=SSD \
  --storage-size=10GB \
  --storage-auto-increase \
  --backup-start-time=03:00 \
  --enable-bin-log \
  --maintenance-window-day=SUN \
  --maintenance-window-hour=4

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
gcloud sql databases create noteapp_prod \
  --instance=noteapp-prod

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
gcloud sql users create noteapp_user \
  --instance=noteapp-prod \
  --password=SECURE_PASSWORD_HERE

# æ¥ç¶šæƒ…å ±ç¢ºèª
gcloud sql instances describe noteapp-prod
```

#### Cloud SQL Proxyè¨­å®š

```bash
# Cloud SQL Proxyã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy
chmod +x cloud_sql_proxy

# æ¥ç¶šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ï¼‰
./cloud_sql_proxy -instances=PROJECT_ID:REGION:noteapp-prod=tcp:5432
```

---

## ğŸ”§ Alembicå°å…¥

### 1. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
cd server
pip install alembic psycopg2-binary
```

**requirements.txt ã«è¿½åŠ **:
```
alembic==1.13.1
psycopg2-binary==2.9.9
```

---

### 2. AlembicåˆæœŸåŒ–

```bash
cd server
alembic init alembic
```

**ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«**:
```
server/
â”œâ”€â”€ alembic/
â”‚   â”œâ”€â”€ env.py               # Alembicç’°å¢ƒè¨­å®š
â”‚   â”œâ”€â”€ script.py.mako       # ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
â”‚   â””â”€â”€ versions/            # ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
â””â”€â”€ alembic.ini              # Alembicè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
```

---

### 3. Alembicè¨­å®š

#### alembic.ini ç·¨é›†

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/alembic.ini`

```ini
[alembic]
script_location = alembic
prepend_sys_path = .

# âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
# sqlalchemy.url = driver://user:pass@localhost/dbname

file_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = INFO
handlers = console

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

#### env.py ç·¨é›†

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/alembic/env.py`

```python
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, os.path.realpath(os.path.join(os.path.dirname(__file__), '..')))

# Alembic Config
config = context.config

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.billing.infrastructure.persistence.database import Base
from src.billing.domain.entities.user import User
from src.billing.domain.entities.device_auth import DeviceAuth
from src.billing.domain.entities.credit import Credit
from src.billing.domain.entities.token_balance import TokenBalance
from src.billing.domain.entities.token_pricing import TokenPricing
from src.billing.domain.entities.transaction import Transaction

target_metadata = Base.metadata

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰DATABASE_URLã‚’å–å¾—
def get_url():
    from src.core.config import get_settings
    settings = get_settings()
    return settings.database_url

# DATABASE_URLã‚’å‹•çš„ã«è¨­å®š
config.set_main_option("sqlalchemy.url", get_url())

def run_migrations_offline() -> None:
    """Offline mode: SQLãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    """Online mode: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç›´æ¥é©ç”¨"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,  # ã‚«ãƒ©ãƒ ã‚¿ã‚¤ãƒ—å¤‰æ›´æ¤œå‡º
            compare_server_default=True,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤å¤‰æ›´æ¤œå‡º
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

---

### 4. åˆæœŸãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ

```bash
cd server

# ç¾åœ¨ã®ã‚¹ã‚­ãƒ¼ãƒã‹ã‚‰åˆæœŸãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
alembic revision --autogenerate -m "Initial migration"
```

**ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«**: `server/alembic/versions/20251121_1430_abc123_initial_migration.py`

**å†…å®¹ä¾‹**:
```python
"""Initial migration

Revision ID: abc123def456
Revises:
Create Date: 2025-11-21 14:30:00.000000
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = 'abc123def456'
down_revision = None
branch_labels = None
depends_on = None

def upgrade() -> None:
    # Users table
    op.create_table(
        'users',
        sa.Column('user_id', sa.String(), nullable=False),
        sa.Column('google_id', sa.String(), nullable=True),
        sa.Column('email', sa.String(), nullable=True),
        sa.Column('display_name', sa.String(), nullable=True),
        sa.Column('profile_picture_url', sa.String(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('user_id'),
        sa.UniqueConstraint('google_id')
    )
    op.create_index('ix_users_email', 'users', ['email'])

    # Device Auth table
    op.create_table(
        'device_auth',
        sa.Column('device_id', sa.String(), nullable=False),
        sa.Column('user_id', sa.String(), nullable=False),
        sa.Column('device_name', sa.String(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('last_used_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['user_id'], ['users.user_id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('device_id')
    )

    # Credits table
    op.create_table(
        'credits',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.String(), nullable=False),
        sa.Column('credits', sa.Integer(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['user_id'], ['users.user_id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('user_id')
    )

    # Token Balances table
    op.create_table(
        'token_balances',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.String(), nullable=False),
        sa.Column('model_id', sa.String(), nullable=False),
        sa.Column('allocated_tokens', sa.BigInteger(), nullable=False),
        sa.Column('consumed_tokens', sa.BigInteger(), nullable=False, default=0),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['user_id'], ['users.user_id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('user_id', 'model_id')
    )

    # Token Pricing table
    op.create_table(
        'token_pricing',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('model_id', sa.String(), nullable=False),
        sa.Column('model_name', sa.String(), nullable=False),
        sa.Column('credit_per_token', sa.Float(), nullable=False),
        sa.Column('capacity_limit', sa.BigInteger(), nullable=True),
        sa.Column('is_active', sa.Boolean(), nullable=False, default=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('model_id')
    )

    # Transactions table
    op.create_table(
        'transactions',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('transaction_id', sa.String(), nullable=False),
        sa.Column('user_id', sa.String(), nullable=False),
        sa.Column('type', sa.String(), nullable=False),
        sa.Column('amount', sa.Integer(), nullable=False),
        sa.Column('description', sa.String(), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['user_id'], ['users.user_id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('transaction_id')
    )
    op.create_index('ix_transactions_user_id', 'transactions', ['user_id'])
    op.create_index('ix_transactions_created_at', 'transactions', ['created_at'])

def downgrade() -> None:
    op.drop_index('ix_transactions_created_at', 'transactions')
    op.drop_index('ix_transactions_user_id', 'transactions')
    op.drop_table('transactions')
    op.drop_table('token_pricing')
    op.drop_table('token_balances')
    op.drop_table('credits')
    op.drop_table('device_auth')
    op.drop_index('ix_users_email', 'users')
    op.drop_table('users')
```

---

### 5. ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨

```bash
# ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
alembic current

# ä¿ç•™ä¸­ã®ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèª
alembic show

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
alembic upgrade head

# å±¥æ­´ç¢ºèª
alembic history

# ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆ1ã¤å‰ï¼‰
alembic downgrade -1

# ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
alembic downgrade abc123def456
```

---

## ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã®æ›´æ–°

### 1. ç’°å¢ƒå¤‰æ•°ã®æ›´æ–°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/.env.development`

```bash
# Before (SQLite)
# DATABASE_URL=sqlite:///./billing.db

# After (PostgreSQL)
DATABASE_URL=postgresql://noteapp_user:noteapp_dev_password@localhost:5432/noteapp_dev
DATABASE_ECHO=false
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20
DATABASE_POOL_TIMEOUT=30
DATABASE_POOL_RECYCLE=3600
```

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/.env.production`ï¼ˆæ–°è¦ä½œæˆï¼‰

```bash
# Production PostgreSQL (Cloud SQL)
DATABASE_URL=postgresql://noteapp_user:${DB_PASSWORD}@/noteapp_prod?host=/cloudsql/PROJECT_ID:REGION:noteapp-prod
DATABASE_ECHO=false
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40
DATABASE_POOL_TIMEOUT=30
DATABASE_POOL_RECYCLE=1800
```

---

### 2. database.py ã®æ›´æ–°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/src/billing/infrastructure/persistence/database.py`

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from src.core.config import get_settings

settings = get_settings()

# PostgreSQLç”¨ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
engine = create_engine(
    settings.database_url,
    echo=settings.database_echo,
    pool_size=settings.database_pool_size,
    max_overflow=settings.database_max_overflow,
    pool_timeout=settings.database_pool_timeout,
    pool_recycle=settings.database_pool_recycle,
    pool_pre_ping=True,  # æ¥ç¶šç¢ºèª
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³å–å¾—ï¼ˆä¾å­˜æ€§æ³¨å…¥ç”¨ï¼‰"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# âš ï¸ å‰Šé™¤ï¼šAlembicãŒç®¡ç†ã™ã‚‹ãŸã‚ä¸è¦
# Base.metadata.create_all(bind=engine)
```

---

### 3. config.py ã®æ›´æ–°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/src/core/config.py`

```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # ... (æ—¢å­˜ã®è¨­å®š)

    # Database Settings
    database_url: str
    database_echo: bool = False
    database_pool_size: int = 10
    database_max_overflow: int = 20
    database_pool_timeout: int = 30
    database_pool_recycle: int = 3600

    class Config:
        env_file = ".env"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    return Settings()
```

---

## ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ç§»è¡Œï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

### SQLiteã‹ã‚‰PostgreSQLã¸ã®ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ

#### æ–¹æ³•1: SQLAlchemyã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/scripts/migrate_data.py`ï¼ˆæ–°è¦ä½œæˆï¼‰

```python
"""SQLite â†’ PostgreSQL ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# SQLiteæ¥ç¶š
sqlite_url = "sqlite:///./billing.db"
sqlite_engine = create_engine(sqlite_url)
SQLiteSession = sessionmaker(bind=sqlite_engine)

# PostgreSQLæ¥ç¶š
postgres_url = os.getenv("DATABASE_URL")
postgres_engine = create_engine(postgres_url)
PostgresSession = sessionmaker(bind=postgres_engine)

def migrate_data():
    """ãƒ‡ãƒ¼ã‚¿ç§»è¡Œå®Ÿè¡Œ"""
    sqlite_session = SQLiteSession()
    postgres_session = PostgresSession()

    try:
        # Usersãƒ†ãƒ¼ãƒ–ãƒ«ç§»è¡Œ
        from src.billing.domain.entities.user import User
        users = sqlite_session.query(User).all()
        for user in users:
            postgres_session.merge(user)

        # Device Authãƒ†ãƒ¼ãƒ–ãƒ«ç§»è¡Œ
        from src.billing.domain.entities.device_auth import DeviceAuth
        devices = sqlite_session.query(DeviceAuth).all()
        for device in devices:
            postgres_session.merge(device)

        # Creditsãƒ†ãƒ¼ãƒ–ãƒ«ç§»è¡Œ
        from src.billing.domain.entities.credit import Credit
        credits = sqlite_session.query(Credit).all()
        for credit in credits:
            postgres_session.merge(credit)

        # Token Balancesãƒ†ãƒ¼ãƒ–ãƒ«ç§»è¡Œ
        from src.billing.domain.entities.token_balance import TokenBalance
        balances = sqlite_session.query(TokenBalance).all()
        for balance in balances:
            postgres_session.merge(balance)

        # Transactionsãƒ†ãƒ¼ãƒ–ãƒ«ç§»è¡Œ
        from src.billing.domain.entities.transaction import Transaction
        transactions = sqlite_session.query(Transaction).all()
        for transaction in transactions:
            postgres_session.merge(transaction)

        postgres_session.commit()
        print("âœ… Data migration completed successfully")

    except Exception as e:
        postgres_session.rollback()
        print(f"âŒ Migration failed: {e}")
        raise
    finally:
        sqlite_session.close()
        postgres_session.close()

if __name__ == "__main__":
    migrate_data()
```

**å®Ÿè¡Œ**:
```bash
cd server
python scripts/migrate_data.py
```

#### æ–¹æ³•2: pgloaderï¼ˆé«˜é€Ÿï¼‰

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**:
```bash
# Ubuntu/Debian
sudo apt-get install pgloader

# macOS
brew install pgloader
```

**è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**: `server/migration.load`ï¼ˆæ–°è¦ä½œæˆï¼‰

```
LOAD DATABASE
     FROM sqlite:///./billing.db
     INTO postgresql://noteapp_user:password@localhost:5432/noteapp_dev

WITH include drop, create tables, create indexes, reset sequences

SET work_mem to '16MB', maintenance_work_mem to '512 MB';
```

**å®Ÿè¡Œ**:
```bash
pgloader migration.load
```

---

## ğŸ” ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¾Œã®æ¤œè¨¼

### 1. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯

**ãƒ•ã‚¡ã‚¤ãƒ«**: `server/scripts/verify_migration.py`ï¼ˆæ–°è¦ä½œæˆï¼‰

```python
"""ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""
from sqlalchemy import create_engine, func
from sqlalchemy.orm import sessionmaker
from src.billing.domain.entities.user import User
from src.billing.domain.entities.credit import Credit
import os

def verify_migration():
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼"""
    engine = create_engine(os.getenv("DATABASE_URL"))
    Session = sessionmaker(bind=engine)
    session = Session()

    try:
        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ç¢ºèª
        user_count = session.query(func.count(User.user_id)).scalar()
        credit_count = session.query(func.count(Credit.id)).scalar()

        print(f"âœ… Users: {user_count}")
        print(f"âœ… Credits: {credit_count}")

        # å¤–éƒ¨ã‚­ãƒ¼æ•´åˆæ€§
        orphaned_credits = session.query(Credit).outerjoin(User).filter(
            User.user_id == None
        ).count()

        if orphaned_credits > 0:
            print(f"âš ï¸ Orphaned credits found: {orphaned_credits}")
        else:
            print("âœ… Foreign key integrity: OK")

        # NULLå€¤ãƒã‚§ãƒƒã‚¯
        null_users = session.query(User).filter(User.user_id == None).count()
        if null_users > 0:
            print(f"âš ï¸ NULL user_ids found: {null_users}")
        else:
            print("âœ… No NULL user_ids")

        print("\nâœ… Migration verification completed")

    finally:
        session.close()

if __name__ == "__main__":
    verify_migration()
```

---

### 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```python
"""ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
import time
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import os

def performance_test():
    engine = create_engine(os.getenv("DATABASE_URL"))
    Session = sessionmaker(bind=engine)

    # åŒæ™‚æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆ
    import concurrent.futures

    def write_test(i):
        session = Session()
        try:
            from src.billing.domain.entities.transaction import Transaction
            from datetime import datetime

            tx = Transaction(
                transaction_id=f"test-{i}",
                user_id="test-user",
                type="test",
                amount=100,
                created_at=datetime.utcnow()
            )
            session.add(tx)
            session.commit()
            return True
        except Exception as e:
            session.rollback()
            return False
        finally:
            session.close()

    start_time = time.time()

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(write_test, range(100)))

    elapsed = time.time() - start_time

    success_count = sum(results)
    print(f"âœ… Completed: {success_count}/100 in {elapsed:.2f}s")
    print(f"âœ… Throughput: {success_count/elapsed:.2f} writes/sec")

if __name__ == "__main__":
    performance_test()
```

---

## ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ•ãƒ­ãƒ¼

### é–‹ç™ºç’°å¢ƒ

```bash
# 1. Docker Composeèµ·å‹•
docker-compose up -d

# 2. ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
alembic upgrade head

# 3. åˆæœŸãƒ‡ãƒ¼ã‚¿æŠ•å…¥ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
python scripts/seed_data.py

# 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
uvicorn src.main:app --reload
```

### æœ¬ç•ªç’°å¢ƒ

```bash
# 1. Cloud SQLã¸æ¥ç¶šç¢ºèª
gcloud sql connect noteapp-prod --user=noteapp_user

# 2. ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾Œï¼‰
alembic upgrade head

# 3. æ¤œè¨¼
python scripts/verify_migration.py

# 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ—ãƒ­ã‚¤
# (Cloud Runç­‰)
```

---

## ğŸ“‹ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### Phase 1: PostgreSQLç’°å¢ƒæ§‹ç¯‰

- [ ] Docker Composeè¨­å®šä½œæˆ
- [ ] PostgreSQLã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•ç¢ºèª
- [ ] æ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ
- [ ] Cloud SQLã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆï¼ˆæœ¬ç•ªï¼‰
- [ ] Cloud SQL Proxyã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### Phase 2: Alembicå°å…¥

- [ ] Alembicã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- [ ] AlembicåˆæœŸåŒ–
- [ ] env.pyè¨­å®š
- [ ] åˆæœŸãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
- [ ] ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨ãƒ†ã‚¹ãƒˆ

### Phase 3: ã‚³ãƒ¼ãƒ‰æ›´æ–°

- [ ] database.pyæ›´æ–°
- [ ] config.pyæ›´æ–°
- [ ] ç’°å¢ƒå¤‰æ•°æ›´æ–°
- [ ] create_allå‰Šé™¤ç¢ºèª

### Phase 4: ãƒ‡ãƒ¼ã‚¿ç§»è¡Œï¼ˆå¿…è¦æ™‚ï¼‰

- [ ] ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
- [ ] ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ç§»è¡Œå®Ÿè¡Œ
- [ ] ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼
- [ ] æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ç§»è¡Œ

### Phase 5: æ¤œè¨¼

- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆåˆæ ¼
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆåˆæ ¼
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆåˆæ ¼
- [ ] åŒæ™‚æ›¸ãè¾¼ã¿ãƒ†ã‚¹ãƒˆåˆæ ¼

---

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ1: ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ç«¶åˆ

**ã‚¨ãƒ©ãƒ¼**:
```
alembic.util.exc.CommandError: Multiple head revisions are present
```

**è§£æ±º**:
```bash
# ãƒãƒ¼ã‚¸ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
alembic merge heads -m "Merge multiple heads"
alembic upgrade head
```

### å•é¡Œ2: æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼

**ã‚¨ãƒ©ãƒ¼**:
```
TimeoutError: QueuePool limit of size 10 overflow 20 reached
```

**è§£æ±º**:
```bash
# ç’°å¢ƒå¤‰æ•°ã§èª¿æ•´
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40
```

### å•é¡Œ3: Cloud SQLæ¥ç¶šã‚¨ãƒ©ãƒ¼

**ã‚¨ãƒ©ãƒ¼**:
```
FATAL: sorry, too many clients already
```

**è§£æ±º**:
- Cloud SQLã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®max_connectionsã‚’å¢—ã‚„ã™
- æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºã‚’èª¿æ•´
- Connection leakã®ç¢ºèª

---

## ğŸ“Š ç§»è¡Œå®Œäº†ã®æˆåŠŸæŒ‡æ¨™

- âœ… å…¨ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨å®Œäº†
- âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§100%
- âœ… å…¨ãƒ†ã‚¹ãƒˆåˆæ ¼
- âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆåˆæ ¼ï¼ˆSQLiteã‚ˆã‚Šé«˜é€Ÿï¼‰
- âœ… åŒæ™‚æ›¸ãè¾¼ã¿100ä»¶æˆåŠŸ
- âœ… æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸ

---

**ä½œæˆæ—¥**: 2025-11-21
**æ¨å®šå®Œäº†**: Phase 1-3ã§5æ—¥ã€Phase 4-5ã§2æ—¥
