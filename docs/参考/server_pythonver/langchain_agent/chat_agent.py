from typing import Dict, Any, List, Optional
from langchain.agents import create_react_agent, AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import Tool

from ..langchain_tools.web_search_tool import WebSearchTool
from ..langchain_tools.file_tool import FileOperationTool


class LangChainChatAgent:
    def __init__(self):
        self.tools = [
            WebSearchTool(),
            FileOperationTool()
        ]

        # ãƒ¡ãƒ¢ãƒªã®è¨­å®š
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
        self.prompt_template = self._create_prompt_template()

        # LLMã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯åˆå›ä½¿ç”¨æ™‚ã«åˆæœŸåŒ–
        self.llm = None
        self.agent_executor = None

    def _create_prompt_template(self) -> PromptTemplate:
        """ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ"""
        template = """ã‚ãªãŸã¯è¦ªåˆ‡ã§æœ‰èƒ½ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚„è¦æ±‚ã«å¯¾ã—ã¦ã€é©åˆ‡ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

TOOLS:
------

You have access to the following tools:

{tools}

To use a tool, please use the following format:

```
Thought: Do I need to use a tool? What should I do?
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
```

When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:

```
Thought: Do I need to use a tool? No
Final Answer: [your response here]
```

ãƒ«ãƒ¼ãƒ«:
1. æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã‚„æœ€æ–°ã®æƒ…å ±ãŒå¿…è¦ãªå ´åˆã¯ã€web_searchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
2. ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãŒå¿…è¦ãªå ´åˆã¯ã€file_operationãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
3. å˜ç´”ãªæŒ¨æ‹¶ã‚„ä¼šè©±ã«ã¯ã€ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã‚ãšã«ç›´æ¥å›ç­”ã—ã¦ãã ã•ã„
4. æ—¥æœ¬èªã§è‡ªç„¶ãªä¼šè©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„

Begin!

Previous conversation history:
{chat_history}

New input: {input}
Thought: {agent_scratchpad}"""

        return PromptTemplate(
            template=template,
            input_variables=["tools", "tool_names", "chat_history", "input", "agent_scratchpad"]
        )

    def _initialize_agent(self, provider: str = "gemini", model: str = "gemini-1.5-flash"):
        """LLMã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            print(f"ğŸ”§ Initializing LLM with provider: {provider}, model: {model}")

            if provider == "gemini":
                self.llm = ChatGoogleGenerativeAI(
                    model=model,
                    temperature=0.1,
                    max_retries=3
                )
                print("âœ… LLM initialized successfully")
            else:
                # ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚‚å¾Œã§è¿½åŠ å¯èƒ½
                raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")

            print(f"ğŸ”§ Creating ReAct agent with {len(self.tools)} tools")
            print(f"ğŸ”§ Available tools: {[tool.name for tool in self.tools]}")

            # ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ
            agent = create_react_agent(
                llm=self.llm,
                tools=self.tools,
                prompt=self.prompt_template
            )
            print("âœ… ReAct agent created successfully")

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ
            self.agent_executor = AgentExecutor(
                agent=agent,
                tools=self.tools,
                memory=self.memory,
                verbose=True,
                max_iterations=5,
                handle_parsing_errors=True
            )
            print("âœ… Agent executor created successfully")

        except Exception as e:
            print(f"âŒ Agent initialization failed: {str(e)}")
            import traceback
            print(f"âŒ Initialization traceback: {traceback.format_exc()}")
            raise

    async def process_chat(
        self,
        message: str,
        provider: str = "gemini",
        model: str = "gemini-1.5-flash",
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†"""
        try:
            print(f"ğŸ¤– LangChainChatAgent: Processing message: {message}")
            print(f"ğŸ¤– Provider: {provider}, Model: {model}")

            # åˆå›ã¾ãŸã¯è¨­å®šå¤‰æ›´æ™‚ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            if self.agent_executor is None:
                print("ğŸ¤– Initializing agent...")
                self._initialize_agent(provider, model)
                print("ğŸ¤– Agent initialized successfully")

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æƒ…å ±ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            enhanced_message = message
            if context and context.get("currentPath"):
                enhanced_message = f"ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {context['currentPath']}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {message}"

            print(f"ğŸ¤– Enhanced message: {enhanced_message}")

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ
            print("ğŸ¤– Invoking agent executor...")
            result = await self.agent_executor.ainvoke({
                "input": enhanced_message
            })

            print(f"ğŸ¤– Agent result: {result}")

            return {
                "message": result["output"],
                "success": True,
                "agent_used": "LangChain ReAct Agent",
                "provider": provider,
                "model": model,
                "intermediate_steps": result.get("intermediate_steps", [])
            }

        except Exception as e:
            import traceback
            print(f"âŒ LangChainChatAgent Error: {str(e)}")
            print(f"âŒ Traceback: {traceback.format_exc()}")
            return {
                "message": f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "provider": provider,
                "model": model
            }

    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
        try:
            messages = self.memory.chat_memory.messages
            history = []
            for msg in messages:
                if hasattr(msg, 'type'):
                    history.append({
                        "type": msg.type,
                        "content": msg.content
                    })
            return history
        except Exception:
            return []

    def clear_memory(self):
        """ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        self.memory.clear()

    def get_status(self) -> Dict[str, Any]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹ã‚’å–å¾—"""
        return {
            "initialized": self.agent_executor is not None,
            "available_tools": [tool.name for tool in self.tools],
            "memory_messages_count": len(self.memory.chat_memory.messages) if self.memory else 0,
            "llm_model": getattr(self.llm, 'model', None) if self.llm else None
        }